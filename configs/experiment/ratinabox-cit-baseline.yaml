# @package _global_
defaults:
  - /data: ratinabox
  - /operator@tau1:
      - projection
  - /operator@tau2:
      - swap
      - projection


project: "Test"


wandb:
  disabled: false
  group: "${data.mode}_${wandb.task}_${train.name}_${data.estimator_cfg.estimator_type}_bs${data.samples}_T${train.T}"
  task: "ratinabox"
  tags: ["test", "${train.name}", "${data.mode}", "${data.estimator_cfg.estimator_type}"]


tau1:
  projection:
    input_dim: 0

tau2:
  swap:
    p: 204  # 2 * n_cells + c_dim = 2 * 100 + 4 (update when changing n_cells!)
    d: 0
  projection:
    input_dim: 0


model:
  _target_: "models.mlp.MLP"
  input_size: 104  # 1 * n_cells + c_dim = 1 * 100 + 4 (update when changing n_cells!)
  hidden_layer_size: [128, 128]
  layer_norm: true
  drop_out: true
  drop_out_p: 0.3
  output_size: ${data.n_cells}
  bias: true


data:
  data_seed: ${train.seed}
  samples: 20
  type: "type2"
  data_path: "/scratch0/zhhe/data/ratinabox"
  n_cells: 100  # Number of cells (determines a_dim=n_cells, b_dim=n_cells)
  c_dim: 4  # Position (x, y) + head direction
  noise_std: 0.1
  max_n_points: 3000
  mode: "online"  # Options: "pseudo_model_x", "online"
  pretrain_samples: 2000  # Only used in pseudo_model_x mode
  estimator_cfg:
    estimator_type: "gmmn"


train:
  name: "ecrt"
  seed: 0
  lr: 0.0005
  earlystopping:
    patience: 10
    delta: 0.0
  epochs: 500
  seqs: 100  # number of mini-batches (limited by max_n_points / samples)
  T: 0  # Warm start number of mini-batches used for training only
  alpha: 0.05  # significance level
  batch_size: ${data.samples}
  l1_lambda: 0.0
  l2_lambda: 0.0
  save: false
  save_dir: ""
