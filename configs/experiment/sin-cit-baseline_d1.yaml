# @package _global_
defaults:
  - /data: sincit
  - /operator@tau1:
      - projection
  - /operator@tau2:
      - swap
      - projection


project: "Test"

wandb:
  disabled: false
  group: "${data.mode}_${wandb.task}_d${data.z_dim}_coord${data.ca_dim_idx}${data.cb_dim_idx}${data.cr_dim_idx}_${train.name}_mlp_bs${data.samples}_T${train.T}"
  task: "sin"
  tags: ["icml", "${train.name}_v1", "${data.mode}"]


tau1:
  projection:
    input_dim: 0

tau2:
  swap:
    p: 3 # data.z_dim + 2
    d: 0
  projection:
    input_dim: 0


model:
  _target_: "models.mlp.MLP"
  input_size: 2 # data.z_dim + 1
  hidden_layer_size: [4, 128, 128, 4]
  layer_norm: true
  drop_out: true
  drop_out_p: 0.3
  output_size: 1
  bias: true

data:
  z_dim: 1
  data_seed: ${train.seed}
  samples: 20
  type: "type2"
  beta: 3
  alpha: 0.1
  mode: "model_x"  # Options: "model_x", "pseudo_model_x", "online"
  pretrain_samples: 3000  # Only used in pseudo_model_x mode
  ca_dim_idx: 0
  cb_dim_idx: 0
  cr_dim_idx: 0

train:
  name: "ecrt"
  seed: 0
  lr: 0.0005
  earlystopping:
    patience: 10
    delta: 0.0
  epochs: 500
  seqs: 100 # number of mini-batches
  T: 0 #Warm start number of mini-batches used for the training only
  alpha: 0.05 # significance level
  batch_size: ${data.samples}
  l1_lambda: 0.0
  l2_lambda: 0.0
  save: false
  save_dir: ""